[bert]
model_name = google-bert/bert-base-uncased
vocab_size = 30522
type_vocab_size = 2
max_position_embeddings = 512
position_embedding_type = absolute
hidden_units = 768
num_layers = 12
num_heads = 12
size_per_head = 64
activation_type = gelu
inter_size = 3072
layer_norm_eps = 1e-12
weight_data_type = fp16
tensor_para_size = 1

